{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name= 'GPT-3.5-turbo',\n",
    "    temperature=0.9,\n",
    "    max_tokens=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"what is Rippey.AI?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_hf = HuggingFaceHub(\n",
    "    repo_id = \"google/flan-t5-large\",\n",
    "    model_kwargs = {\"temperature\": 0.9, \"max_length\": 300}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rebanaryal/langchain-basics/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer\n"
     ]
    }
   ],
   "source": [
    "text = \"what is Rippey AI?\"\n",
    "print(llm_hf(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_template =  \"\"\"I want you to act as a naming consultant for new restaurants.\n",
    "\n",
    "Return a list of restaurant names. Each name should be short, catchy and easy to remember. It shoud relate to the type of restaurant you are naming.\n",
    "\n",
    "What are some good names for a restaurant that is {restaurant_desription}?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"restaurant_description\"],\n",
    "    template = restaurant_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example prompt with one input variable\n",
    "prompt_template = PromptTemplate(input_variables= [\"restaurant_description\"], template=restaurant_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want you to act as a naming consultant for new restaurants.\\n\\nReturn a list of restaurant names. Each name should be short, catchy and easy to remember. It shoud relate to the type of restaurant you are naming.\\n\\nWhat are some good names for a restaurant that is a Greek place that serves fresh lamb souvlakis and other Greek food ?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description = \"a Greek place that serves fresh lamb souvlakis and other Greek food \"\n",
    "description_02 = \"a burger place that is themed with baseball memorabilia\"\n",
    "description_03 = \"a cafe that has live hard rock music and memorabilia\"\n",
    "\n",
    "## to see what the prompt will be like\n",
    "prompt_template.format(restaurant_desription=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby shabby \n"
     ]
    }
   ],
   "source": [
    "## querying the model with model templates\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm_hf, prompt = prompt_template)\n",
    "print(chain.run(description_03))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Few shots template and prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'happy', 'antonyms': 'sad'}, {'word': 'tall', 'antonyms': 'short'}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, create a list of few shot examples\n",
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonyms\": \"sad\"},\n",
    "    {\"word\": \"tall\",  \"antonyms\":\"short\"}\n",
    "    \n",
    "]\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we specify the template to format the examples we have provided.\n",
    "\n",
    "example_formatter_template = \"\"\" \n",
    "Word: {word}\n",
    "Antonym: {antonyms} \\n\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(input_variable = [\"word\", \"antonym\"],\n",
    "                                template = example_formatter_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we create the `FewShotPromptTemplate` object.\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    # These are the examples we want to insert into the prompt.\n",
    "    examples=examples,\n",
    "    # This is how we want to format the examples when we insert them into the prompt.\n",
    "    example_prompt=example_prompt,\n",
    "    # The prefix is some text that goes before the examples in the prompt.\n",
    "    # Usually, this consists of intructions.\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    # The suffix is some text that goes after the examples in the prompt.\n",
    "    # Usually, this is where the user input will go\n",
    "    suffix=\"Word: {input}\\nAntonym:\",\n",
    "    # The input variables are the variables that the overall prompt expects.\n",
    "    input_variables=[\"input\"],\n",
    "    # The example_separator is the string we will use to join the prefix, examples, and suffix together with.\n",
    "    example_separator=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      " \n",
      "Word: happy\n",
      "Antonym: sad \n",
      "\n",
      "\n",
      " \n",
      "Word: tall\n",
      "Antonym: short \n",
      "\n",
      "\n",
      "Word: slow\n",
      "Antonym:\n"
     ]
    }
   ],
   "source": [
    "# We can now generate a prompt using the `format` method.\n",
    "print(few_shot_prompt.format(input=\"slow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm_hf, prompt=few_shot_prompt)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "print(chain.run(\"slow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
