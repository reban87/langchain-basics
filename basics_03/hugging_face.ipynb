{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace\n",
    "There are two HuggingFace LLM wrappers, one for a local pipeline and one for a model hosted on HuggingFaceHub. Note that wrappers only work for models that support the following tasks:\n",
    "- text2text generation\n",
    "- text - generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the HUGGINGFACEHUB\n",
    "import huggingface_hub\n",
    "from langchain import HuggingFaceHub, PromptTemplate, LLMChain\n",
    "template = \"\"\"\n",
    "Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt,\n",
    "                     llm=HuggingFaceHub(\n",
    "                         repo_id=\"google/flan-t5-large\",\n",
    "                         model_kwargs={\"temperature\":0,\n",
    "                                       \"max_length\":64},\n",
    "                     ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris. Paris is the capital of France. So, the answer is Paris.\n"
     ]
    }
   ],
   "source": [
    "question = \"what is the capital of France?\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris is a city in France. Paris is a city in Europe. European cities are known for their beautiful girls. So the answer is yes.\n"
     ]
    }
   ],
   "source": [
    "question = \"If i go to Paris, will I get proposed by a beautiful girl?\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading HF model locally and run it as a LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'google/flan-t5-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=250\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline( pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kathmandu\n"
     ]
    }
   ],
   "source": [
    "print(local_llm(\"what is the capital of Nepal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Prime Minister of Nepal is the leader of the country. The Prime Minister of Nepal is the leader of the country. So, the answer is the Prime Minister of Nepal.\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(\n",
    "    prompt = prompt,\n",
    "    llm = local_llm\n",
    ")\n",
    "question = \"who is the Prime Minister of Nepal\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2-medium - Decoder Only Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model_id = \"gpt2-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rebanaryal/langchain-basics/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    max_length = 250\n",
    ")\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rebanaryal/langchain-basics/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: what is the capital of Nepal\n",
      "Answer: Let's think step by step.\n",
      "        Kathmandu, the capital, has the capital of the most populous region in the whole world, and is almost entirely independent of the country ruled by a central government, called Nepal. It is a province, and this is how government officials who speak their own language are referred to in this country.\n",
      "        Nepal's capital, Kathmandu, is also the capital for the entire country.\n",
      "        The current president of Nepal, the Nepalese Prime Minister Sirleaf (who won re-election to her second term in June 2012)\n",
      "        is the president of Nepal. The parliament of Nepal is an assembly chamber which holds 30 votes, held during a national parliamentary election (they were formed from elections taken in 2001). The president presides. All other members of the parliament also come from a central government.\n",
      "        The national flag is called a 'hindu' flag. Sirleaf is a descendant of 'Latha Brahman', Hinduism's founder.\n",
      "        Sirleaf is the first woman Prime Minister in the history of Nepal.\n",
      "        Sirleaf is very conservative in her views of\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=local_llm,\n",
    "    prompt = prompt\n",
    ")\n",
    "\n",
    "question = \"what is the capital of Nepal\"\n",
    "print(llm_chain.run(question))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
